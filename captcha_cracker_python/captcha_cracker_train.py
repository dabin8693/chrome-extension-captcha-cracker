import os
import cv2
from glob import glob
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.utils import plot_model


if __name__ == '__main__':

    #Preview Dataset
    img_list = glob('sample/test_numbers_2/*.jpg')
    len(img_list)

    label = os.path.splitext(os.path.basename(img_list[0]))[0]

    img = cv2.imread(img_list[0])
    plt.title(label)
    plt.imshow(img)

    #Preprocessing
    imgs = []
    labels = []
    x_list = []
    max_length = 0

    for img_path in img_list:
        imgs.append(img_path)

        label = os.path.splitext(os.path.basename(img_path))[0]
        labels.append(label)

        if len(label) > max_length:
            max_length = len(label)



    ''.join(labels)
    characters = set(''.join(labels))
    characters2 = ['0','1','2','3','4','5','6','7','8','9']

    #Encode Labels

    char_to_num = layers.experimental.preprocessing.StringLookup(
        vocabulary=characters2, num_oov_indices=0, mask_token=None
    )

    num_to_char = layers.experimental.preprocessing.StringLookup(
        vocabulary=char_to_num.get_vocabulary(), num_oov_indices=0, mask_token=None, invert=True
    )

    encoded = char_to_num(tf.strings.unicode_split(labels[0], input_encoding='UTF-8'))

    tf.strings.reduce_join(num_to_char(encoded)).numpy().decode('utf-8')

    #Split Dataset
    from sklearn.model_selection import train_test_split

    x_train, x_val, y_train, y_val = train_test_split(imgs, labels, test_size=0.3, random_state=2021)


    #Create Data Generator
    img_width = 200
    img_height = 50


    def encode_single_sample(img_path, label):
        # 1. Read image
        img = tf.io.read_file(img_path)
        # 2. Decode and convert to grayscale
        img = tf.io.decode_png(img, channels=1)
        # 3. Convert to float32 in [0, 1] range
        img = tf.image.convert_image_dtype(img, tf.float32)
        # 4. Resize to the desired size
        img = tf.image.resize(img, [img_height, img_width])
        # 5. Transpose the image because we want the time
        # dimension to correspond to the width of the image.
        img = tf.transpose(img, perm=[1, 0, 2])
        # 6. Map the characters in label to numbers
        label = char_to_num(tf.strings.unicode_split(label, input_encoding='UTF-8'))
        # 7. Return a dict as our model is expecting two inputs
        return {'image': img, 'label': label}


    preview = encode_single_sample(imgs[0], labels[0])


    batch_size = 64

    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
    train_dataset = (
        train_dataset.map(
            encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE
        )
        .batch(batch_size)
        .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    )

    validation_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))
    validation_dataset = (
        validation_dataset.map(
            encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE
        )
        .batch(batch_size)
        .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    )


    #Model
    class CTCLayer(layers.Layer):
        def __init__(self, name=None):
            super().__init__(name=name)
            self.loss_fn = keras.backend.ctc_batch_cost

        def call(self, y_true, y_pred):
            # Compute the training-time loss value and add it
            # to the layer using `self.add_loss()`.
            batch_len = tf.cast(tf.shape(y_true)[0], dtype='int64')
            input_length = tf.cast(tf.shape(y_pred)[1], dtype='int64')
            label_length = tf.cast(tf.shape(y_true)[1], dtype='int64')

            input_length = input_length * tf.ones(shape=(batch_len, 1), dtype='int64')
            label_length = label_length * tf.ones(shape=(batch_len, 1), dtype='int64')

            loss = self.loss_fn(y_true, y_pred, input_length, label_length)
            self.add_loss(loss)

            # At test time, just return the computed predictions
            return y_pred


    def build_model():
        # Inputs to the model
        input_img = layers.Input(
            shape=(img_width, img_height, 1), name='image', dtype='float32'
        )
        labels = layers.Input(name='label', shape=(None,), dtype='float32')


        # First conv block
        x = layers.Conv2D(
            24,
            (3, 3),
            activation='relu',
            kernel_initializer='he_normal',
            padding='same',
            name='Conv1',
        )(input_img)
        x = layers.MaxPooling2D((2, 2), name='pool1')(x)

        # Second conv block
        x = layers.Conv2D(
            48,
            (3, 3),
            activation='relu',
            kernel_initializer='he_normal',
            padding='same',
            name='Conv2',
        )(x)
        x = layers.MaxPooling2D((2, 2), name='pool2')(x)





        # We have used two max pool with pool size and strides 2.
        # Hence, downsampled feature maps are 4x smaller. The number of
        # filters in the last layer is 64. Reshape accordingly before
        # passing the output to the RNN part of the model

        new_shape = ((img_width // 4), (img_height // 4) * 48)
        x = layers.Reshape(target_shape=new_shape, name='reshape')(x)
        x = layers.Dense(128, activation='relu', name='dense1')(x)
        x = layers.Dropout(0.25)(x)

        # RNNs
        x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.25))(x)
        x = layers.Bidirectional(layers.LSTM(32, return_sequences=True, dropout=0.25))(x)

        # Output layer
        x = layers.Dense(
            len(char_to_num.get_vocabulary()) + 1, activation='softmax', name='dense2'
        )(x)

        # Add CTC layer for calculating CTC loss at each step
        output = CTCLayer(name='ctc_loss')(labels, x)


        # Define the model
        model = keras.models.Model(
            inputs=[input_img, labels], outputs=output, name='ocr_model_v1'
        )
        # Optimizer
        opt = keras.optimizers.Adam()
        # Compile the model and return
        model.compile(optimizer=opt)
        return model


    # Get the model
    model = build_model()
    model.summary()
    plot_model(model, show_shapes=True, to_file='model.png')

    #Train
    early_stopping = keras.callbacks.EarlyStopping(
        monitor='val_loss', patience=75, restore_best_weights=True
    )

    history = model.fit(
        train_dataset,
        validation_data=validation_dataset,
        epochs=500,
        callbacks=[early_stopping],
    )

    model.save("capcha_cracker_model_test1", save_format='tf')


    # Test Inference
    prediction_model = keras.models.Model(
        model.get_layer(name='image').input, model.get_layer(name='dense2').output
    )


    def decode_batch_predictions(pred):
        input_len = np.ones(pred.shape[0]) * pred.shape[1]
        # Use greedy search. For complex tasks, you can use beam search
        results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][
                  :, :max_length
                  ]
        # Iterate over the results and get back the text
        output_text = []
        for res in results:
            res = tf.strings.reduce_join(num_to_char(res)).numpy().decode('utf-8')
            output_text.append(res)
        return output_text



    for batch in validation_dataset.take(1):


        batch_images = batch['image']
        preds = prediction_model.predict(batch_images)
        pred_texts = decode_batch_predictions(preds)
        _, axes = plt.subplots(8, 4, figsize=(16, 12))
        index = 0
        for img, text in zip(batch_images, pred_texts):
            img = img.numpy().squeeze()
            img = img.T
            row = index % 8
            col = index // 8
            axes[row][col].imshow(img, cmap='gray')
            axes[row][col].set_title(text)
            axes[row][col].set_axis_off()
            index += 1
            if index > 31:
                index = 0
                plt.show()
                _, axes = plt.subplots(8, 4, figsize=(16, 12))

    plt.show()

